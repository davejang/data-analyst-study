{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.\n",
      "  1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.\n",
      " 12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.\n",
      "  9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]\n",
      "이 숫자는  3 입니다.\n"
     ]
    }
   ],
   "source": [
    "# 필기 숫자 데이터 시각화 및 타겟(레이블) 출력\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digit = datasets.load_digits()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(digit.images[3],cmap=plt.cm.gray_r,interpolation='nearest')\n",
    "\n",
    "plt.show()\n",
    "print(digit.data[3])\n",
    "print('이 숫자는 ',digit.target[3],\"입니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터셋은 8x8 픽셀 이미지를 64차원 특징 벡터로 표현한 데이터이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측값은  [0 1 2]\n",
      "실제 값은  0 1 2\n"
     ]
    }
   ],
   "source": [
    "# 실제 숫자 인식 구현\n",
    "from sklearn import svm\n",
    "\n",
    "s = svm.SVC(gamma=0.1,C=10)\n",
    "s.fit(digit.data,digit.target)\n",
    "\n",
    "# sample data를 필기 숫자 데이터셋 앞의 3개로 선정 후 인식\n",
    "new_d = [digit.data[0],digit.data[1],digit.data[2]] # 0, 1, 2\n",
    "res = s.predict(new_d)\n",
    "print(\"예측값은 \",res)\n",
    "print(\"실제 값은 \",digit.target[0],digit.target[1],digit.target[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<참고>\n",
    "sklearn 디자인 패턴\n",
    "\n",
    "데이터 읽기 -> 모델 객체 생성 -> 모델 학습 -> 에측 -> 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[71.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 64.  0.  0.  0.  0.  1.  0.  1.  1.]\n",
      " [ 0.  0. 75.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 69.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0. 73.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0. 65.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  1. 70.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. 76.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0. 63.  1.]\n",
      " [ 0.  0.  0.  0.  0.  2.  0.  1.  0. 82.]]\n",
      "accuracy = 98.47009735744089\n",
      "[0.975      0.95       0.98328691 0.99164345 0.96100279]\n",
      "정확률(평균)=97.219,표춘편차=0.015\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋은 훈련 집합과 테스트 집합으로 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6) # 훈련 데이터셋 비율 60%, 테스트 데이터셋 비울 40%\n",
    "\n",
    "# SVM을 통해 학습 및 예측\n",
    "s = svm.SVC(gamma=0.001)\n",
    "s.fit(x_train,y_train)\n",
    "res = s.predict(x_test)\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "conf = np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# accuracy 측정\n",
    "not_correct = 0\n",
    "for i in range(10):\n",
    "    not_correct+=conf[i][i]\n",
    "accuracy = not_correct/len(res)\n",
    "print(\"accuracy =\",accuracy*100)\n",
    "\n",
    "# cross_val_score을 활용한 교차 검증\n",
    "accuracies = cross_val_score(s,digit.data,digit.target,cv=5) # 5겹 교차 검증\n",
    "print(accuracies)\n",
    "print(\"정확률(평균)=%0.3f,표춘편차=%0.3f\"%(accuracies.mean()*100,accuracies.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[76.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 58.  0.  0.  0.  0.  0.  0.  2.  0.]\n",
      " [ 0.  0. 66.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  3.  0. 73.  0.  0.  0.  1.  2.  4.]\n",
      " [ 0.  0.  0.  0. 66.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0. 65.  1.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0. 71.  0.  2.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. 73.  0.  0.]\n",
      " [ 1.  9.  1.  3.  2.  1.  0.  1. 64.  5.]\n",
      " [ 0.  1.  0.  2.  0.  1.  0.  1.  0. 63.]]\n",
      "accuracy = 93.88038942976355\n",
      "[0.89722222 0.84444444 0.92200557 0.94428969 0.87465181]\n",
      "정확률(평균)=89.652,표춘편차=0.035\n"
     ]
    }
   ],
   "source": [
    "# 단일 perceptron을 사용해 필기 숫자 예측\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋은 훈련 집합과 테스트 집합으로 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6) # 훈련 데이터셋 비율 60%, 테스트 데이터셋 비울 40%\n",
    "\n",
    "# SVM을 통해 학습 및 예측\n",
    "p = Perceptron(max_iter=100,eta0=0.001,verbose=0)\n",
    "p.fit(x_train,y_train)\n",
    "res = p.predict(x_test)\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "conf = np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# accuracy 측정\n",
    "not_correct = 0\n",
    "for i in range(10):\n",
    "    not_correct+=conf[i][i]\n",
    "accuracy = not_correct/len(res)\n",
    "print(\"accuracy =\",accuracy*100)\n",
    "\n",
    "# cross_val_score을 활용한 교차 검증\n",
    "accuracies = cross_val_score(p,digit.data,digit.target,cv=5) # 5겹 교차 검증\n",
    "print(accuracies)\n",
    "print(\"정확률(평균)=%0.3f,표춘편차=%0.3f\"%(accuracies.mean()*100,accuracies.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단층 퍼셉트론은 선형 분류기이기 때문에 낮은 정확률을 보여줌\n",
    "-> XOR 같은 데이터에 대한 처리가 불가능하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.30654803\n",
      "Iteration 2, loss = 0.34703149\n",
      "Iteration 3, loss = 0.21573469\n",
      "Iteration 4, loss = 0.17307793\n",
      "Iteration 5, loss = 0.13812685\n",
      "Iteration 6, loss = 0.11947007\n",
      "Iteration 7, loss = 0.10169609\n",
      "Iteration 8, loss = 0.08880758\n",
      "Iteration 9, loss = 0.07829721\n",
      "Iteration 10, loss = 0.07054250\n",
      "Iteration 11, loss = 0.06433784\n",
      "Iteration 12, loss = 0.06181510\n",
      "Iteration 13, loss = 0.05507726\n",
      "Iteration 14, loss = 0.04916909\n",
      "Iteration 15, loss = 0.04912268\n",
      "Iteration 16, loss = 0.04309750\n",
      "Iteration 17, loss = 0.03979549\n",
      "Iteration 18, loss = 0.03672926\n",
      "Iteration 19, loss = 0.03590550\n",
      "Iteration 20, loss = 0.03301624\n",
      "Iteration 21, loss = 0.03299229\n",
      "Iteration 22, loss = 0.02981045\n",
      "Iteration 23, loss = 0.02855113\n",
      "Iteration 24, loss = 0.02647286\n",
      "Iteration 25, loss = 0.02556942\n",
      "Iteration 26, loss = 0.02406559\n",
      "Iteration 27, loss = 0.02273712\n",
      "Iteration 28, loss = 0.02262932\n",
      "Iteration 29, loss = 0.02101120\n",
      "Iteration 30, loss = 0.02034660\n",
      "Iteration 31, loss = 0.02016324\n",
      "Iteration 32, loss = 0.01856356\n",
      "Iteration 33, loss = 0.01802703\n",
      "Iteration 34, loss = 0.01754754\n",
      "Iteration 35, loss = 0.01732743\n",
      "Iteration 36, loss = 0.01656005\n",
      "Iteration 37, loss = 0.01549701\n",
      "Iteration 38, loss = 0.01532801\n",
      "Iteration 39, loss = 0.01469650\n",
      "Iteration 40, loss = 0.01461551\n",
      "Iteration 41, loss = 0.01398086\n",
      "Iteration 42, loss = 0.01365625\n",
      "Iteration 43, loss = 0.01304379\n",
      "Iteration 44, loss = 0.01280687\n",
      "Iteration 45, loss = 0.01231147\n",
      "Iteration 46, loss = 0.01231463\n",
      "Iteration 47, loss = 0.01201450\n",
      "Iteration 48, loss = 0.01166014\n",
      "Iteration 49, loss = 0.01124694\n",
      "Iteration 50, loss = 0.01112175\n",
      "Iteration 51, loss = 0.01076360\n",
      "Iteration 52, loss = 0.01060086\n",
      "Iteration 53, loss = 0.01027908\n",
      "Iteration 54, loss = 0.01009528\n",
      "Iteration 55, loss = 0.00988947\n",
      "Iteration 56, loss = 0.00958634\n",
      "Iteration 57, loss = 0.00970864\n",
      "Iteration 58, loss = 0.00949901\n",
      "Iteration 59, loss = 0.00927367\n",
      "Iteration 60, loss = 0.00893321\n",
      "Iteration 61, loss = 0.00894767\n",
      "Iteration 62, loss = 0.00868837\n",
      "Iteration 63, loss = 0.00854101\n",
      "Iteration 64, loss = 0.00836588\n",
      "Iteration 65, loss = 0.00822957\n",
      "Iteration 66, loss = 0.00809494\n",
      "Iteration 67, loss = 0.00787031\n",
      "Iteration 68, loss = 0.00801291\n",
      "Iteration 69, loss = 0.00769616\n",
      "Iteration 70, loss = 0.00763401\n",
      "Iteration 71, loss = 0.00746014\n",
      "Iteration 72, loss = 0.00727689\n",
      "Iteration 73, loss = 0.00728499\n",
      "Iteration 74, loss = 0.00714316\n",
      "Iteration 75, loss = 0.00705330\n",
      "Iteration 76, loss = 0.00692888\n",
      "Iteration 77, loss = 0.00688246\n",
      "Iteration 78, loss = 0.00674607\n",
      "Iteration 79, loss = 0.00656978\n",
      "Iteration 80, loss = 0.00653765\n",
      "Iteration 81, loss = 0.00640273\n",
      "Iteration 82, loss = 0.00638229\n",
      "Iteration 83, loss = 0.00634093\n",
      "Iteration 84, loss = 0.00614856\n",
      "Iteration 85, loss = 0.00610865\n",
      "Iteration 86, loss = 0.00605044\n",
      "Iteration 87, loss = 0.00598460\n",
      "Iteration 88, loss = 0.00588225\n",
      "Iteration 89, loss = 0.00582383\n",
      "Iteration 90, loss = 0.00576945\n",
      "Iteration 91, loss = 0.00572972\n",
      "Iteration 92, loss = 0.00563936\n",
      "Iteration 93, loss = 0.00555071\n",
      "Iteration 94, loss = 0.00544682\n",
      "Iteration 95, loss = 0.00542393\n",
      "Iteration 96, loss = 0.00538034\n",
      "Iteration 97, loss = 0.00537471\n",
      "Iteration 98, loss = 0.00521446\n",
      "Iteration 99, loss = 0.00522156\n",
      "Iteration 100, loss = 0.00514777\n",
      "Iteration 101, loss = 0.00510149\n",
      "Iteration 102, loss = 0.00500183\n",
      "Iteration 103, loss = 0.00496707\n",
      "Iteration 104, loss = 0.00495969\n",
      "Iteration 105, loss = 0.00486552\n",
      "Iteration 106, loss = 0.00480643\n",
      "Iteration 107, loss = 0.00479369\n",
      "Iteration 108, loss = 0.00474619\n",
      "Iteration 109, loss = 0.00465361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[64.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0. 66.  0.  1.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0. 67.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  2.  0. 78.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0. 78.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0. 74.  1.  0.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  1. 71.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0. 68.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1. 62.  2.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.  0.  0. 72.]]\n",
      "accuracy = 97.35744089012516\n",
      "Iteration 1, loss = 1.97279324\n",
      "Iteration 2, loss = 0.29594229\n",
      "Iteration 3, loss = 0.20292854\n",
      "Iteration 4, loss = 0.15493493\n",
      "Iteration 5, loss = 0.12166683\n",
      "Iteration 6, loss = 0.10595675\n",
      "Iteration 7, loss = 0.09010742\n",
      "Iteration 8, loss = 0.07969632\n",
      "Iteration 9, loss = 0.07120360\n",
      "Iteration 10, loss = 0.06240043\n",
      "Iteration 11, loss = 0.05843014\n",
      "Iteration 12, loss = 0.05212187\n",
      "Iteration 13, loss = 0.04718264\n",
      "Iteration 14, loss = 0.04310779\n",
      "Iteration 15, loss = 0.04049830\n",
      "Iteration 16, loss = 0.03725464\n",
      "Iteration 17, loss = 0.03463809\n",
      "Iteration 18, loss = 0.03232773\n",
      "Iteration 19, loss = 0.03076264\n",
      "Iteration 20, loss = 0.02888619\n",
      "Iteration 21, loss = 0.02695195\n",
      "Iteration 22, loss = 0.02536268\n",
      "Iteration 23, loss = 0.02347804\n",
      "Iteration 24, loss = 0.02270954\n",
      "Iteration 25, loss = 0.02239884\n",
      "Iteration 26, loss = 0.02041293\n",
      "Iteration 27, loss = 0.01929528\n",
      "Iteration 28, loss = 0.01883452\n",
      "Iteration 29, loss = 0.01818232\n",
      "Iteration 30, loss = 0.01658129\n",
      "Iteration 31, loss = 0.01673438\n",
      "Iteration 32, loss = 0.01578484\n",
      "Iteration 33, loss = 0.01556116\n",
      "Iteration 34, loss = 0.01440015\n",
      "Iteration 35, loss = 0.01403335\n",
      "Iteration 36, loss = 0.01362113\n",
      "Iteration 37, loss = 0.01374663\n",
      "Iteration 38, loss = 0.01288402\n",
      "Iteration 39, loss = 0.01258631\n",
      "Iteration 40, loss = 0.01197719\n",
      "Iteration 41, loss = 0.01155489\n",
      "Iteration 42, loss = 0.01136800\n",
      "Iteration 43, loss = 0.01149477\n",
      "Iteration 44, loss = 0.01080857\n",
      "Iteration 45, loss = 0.01083138\n",
      "Iteration 46, loss = 0.01074901\n",
      "Iteration 47, loss = 0.01001784\n",
      "Iteration 48, loss = 0.00980092\n",
      "Iteration 49, loss = 0.00941662\n",
      "Iteration 50, loss = 0.00935100\n",
      "Iteration 51, loss = 0.00909203\n",
      "Iteration 52, loss = 0.00895202\n",
      "Iteration 53, loss = 0.00882385\n",
      "Iteration 54, loss = 0.00863596\n",
      "Iteration 55, loss = 0.00841402\n",
      "Iteration 56, loss = 0.00831998\n",
      "Iteration 57, loss = 0.00813891\n",
      "Iteration 58, loss = 0.00788004\n",
      "Iteration 59, loss = 0.00759927\n",
      "Iteration 60, loss = 0.00755493\n",
      "Iteration 61, loss = 0.00740260\n",
      "Iteration 62, loss = 0.00730288\n",
      "Iteration 63, loss = 0.00710981\n",
      "Iteration 64, loss = 0.00704063\n",
      "Iteration 65, loss = 0.00687700\n",
      "Iteration 66, loss = 0.00683953\n",
      "Iteration 67, loss = 0.00676510\n",
      "Iteration 68, loss = 0.00670434\n",
      "Iteration 69, loss = 0.00651596\n",
      "Iteration 70, loss = 0.00632169\n",
      "Iteration 71, loss = 0.00636051\n",
      "Iteration 72, loss = 0.00612303\n",
      "Iteration 73, loss = 0.00618755\n",
      "Iteration 74, loss = 0.00596788\n",
      "Iteration 75, loss = 0.00598647\n",
      "Iteration 76, loss = 0.00591157\n",
      "Iteration 77, loss = 0.00576515\n",
      "Iteration 78, loss = 0.00568594\n",
      "Iteration 79, loss = 0.00561272\n",
      "Iteration 80, loss = 0.00556206\n",
      "Iteration 81, loss = 0.00546820\n",
      "Iteration 82, loss = 0.00532304\n",
      "Iteration 83, loss = 0.00521171\n",
      "Iteration 84, loss = 0.00517440\n",
      "Iteration 85, loss = 0.00513379\n",
      "Iteration 86, loss = 0.00501522\n",
      "Iteration 87, loss = 0.00499125\n",
      "Iteration 88, loss = 0.00494188\n",
      "Iteration 89, loss = 0.00492877\n",
      "Iteration 90, loss = 0.00478871\n",
      "Iteration 91, loss = 0.00474161\n",
      "Iteration 92, loss = 0.00466557\n",
      "Iteration 93, loss = 0.00462176\n",
      "Iteration 94, loss = 0.00462266\n",
      "Iteration 95, loss = 0.00456463\n",
      "Iteration 96, loss = 0.00448197\n",
      "Iteration 97, loss = 0.00441527\n",
      "Iteration 98, loss = 0.00435882\n",
      "Iteration 99, loss = 0.00432136\n",
      "Iteration 100, loss = 0.00428332\n",
      "Iteration 101, loss = 0.00426904\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.84037418\n",
      "Iteration 2, loss = 0.26877204\n",
      "Iteration 3, loss = 0.17781882\n",
      "Iteration 4, loss = 0.14238269\n",
      "Iteration 5, loss = 0.11838848\n",
      "Iteration 6, loss = 0.09509701\n",
      "Iteration 7, loss = 0.08366546\n",
      "Iteration 8, loss = 0.07167490\n",
      "Iteration 9, loss = 0.06198553\n",
      "Iteration 10, loss = 0.05663107\n",
      "Iteration 11, loss = 0.05067111\n",
      "Iteration 12, loss = 0.04542855\n",
      "Iteration 13, loss = 0.04266783\n",
      "Iteration 14, loss = 0.03908427\n",
      "Iteration 15, loss = 0.03653196\n",
      "Iteration 16, loss = 0.03401732\n",
      "Iteration 17, loss = 0.03281442\n",
      "Iteration 18, loss = 0.03159211\n",
      "Iteration 19, loss = 0.02756804\n",
      "Iteration 20, loss = 0.02770740\n",
      "Iteration 21, loss = 0.02499099\n",
      "Iteration 22, loss = 0.02431340\n",
      "Iteration 23, loss = 0.02311224\n",
      "Iteration 24, loss = 0.02276729\n",
      "Iteration 25, loss = 0.02052763\n",
      "Iteration 26, loss = 0.02025750\n",
      "Iteration 27, loss = 0.01772219\n",
      "Iteration 28, loss = 0.01913838\n",
      "Iteration 29, loss = 0.01690687\n",
      "Iteration 30, loss = 0.01772750\n",
      "Iteration 31, loss = 0.01614949\n",
      "Iteration 32, loss = 0.01535887\n",
      "Iteration 33, loss = 0.01451825\n",
      "Iteration 34, loss = 0.01393179\n",
      "Iteration 35, loss = 0.01362136\n",
      "Iteration 36, loss = 0.01351450\n",
      "Iteration 37, loss = 0.01309256\n",
      "Iteration 38, loss = 0.01256994\n",
      "Iteration 39, loss = 0.01252560\n",
      "Iteration 40, loss = 0.01188360\n",
      "Iteration 41, loss = 0.01174827\n",
      "Iteration 42, loss = 0.01096946\n",
      "Iteration 43, loss = 0.01105446\n",
      "Iteration 44, loss = 0.01097476\n",
      "Iteration 45, loss = 0.01024035\n",
      "Iteration 46, loss = 0.01017006\n",
      "Iteration 47, loss = 0.01014483\n",
      "Iteration 48, loss = 0.00970823\n",
      "Iteration 49, loss = 0.00946505\n",
      "Iteration 50, loss = 0.00924714\n",
      "Iteration 51, loss = 0.00905008\n",
      "Iteration 52, loss = 0.00869572\n",
      "Iteration 53, loss = 0.00859691\n",
      "Iteration 54, loss = 0.00829710\n",
      "Iteration 55, loss = 0.00812281\n",
      "Iteration 56, loss = 0.00812591\n",
      "Iteration 57, loss = 0.00779795\n",
      "Iteration 58, loss = 0.00764888\n",
      "Iteration 59, loss = 0.00758286\n",
      "Iteration 60, loss = 0.00765274\n",
      "Iteration 61, loss = 0.00727442\n",
      "Iteration 62, loss = 0.00721361\n",
      "Iteration 63, loss = 0.00705547\n",
      "Iteration 64, loss = 0.00694522\n",
      "Iteration 65, loss = 0.00679296\n",
      "Iteration 66, loss = 0.00669143\n",
      "Iteration 67, loss = 0.00662492\n",
      "Iteration 68, loss = 0.00648282\n",
      "Iteration 69, loss = 0.00639949\n",
      "Iteration 70, loss = 0.00624888\n",
      "Iteration 71, loss = 0.00612803\n",
      "Iteration 72, loss = 0.00602629\n",
      "Iteration 73, loss = 0.00593115\n",
      "Iteration 74, loss = 0.00583119\n",
      "Iteration 75, loss = 0.00580359\n",
      "Iteration 76, loss = 0.00578238\n",
      "Iteration 77, loss = 0.00559771\n",
      "Iteration 78, loss = 0.00557862\n",
      "Iteration 79, loss = 0.00544879\n",
      "Iteration 80, loss = 0.00542315\n",
      "Iteration 81, loss = 0.00536749\n",
      "Iteration 82, loss = 0.00531963\n",
      "Iteration 83, loss = 0.00518294\n",
      "Iteration 84, loss = 0.00514698\n",
      "Iteration 85, loss = 0.00513136\n",
      "Iteration 86, loss = 0.00505770\n",
      "Iteration 87, loss = 0.00496441\n",
      "Iteration 88, loss = 0.00486716\n",
      "Iteration 89, loss = 0.00485384\n",
      "Iteration 90, loss = 0.00475914\n",
      "Iteration 91, loss = 0.00477732\n",
      "Iteration 92, loss = 0.00462795\n",
      "Iteration 93, loss = 0.00457620\n",
      "Iteration 94, loss = 0.00465467\n",
      "Iteration 95, loss = 0.00450881\n",
      "Iteration 96, loss = 0.00446781\n",
      "Iteration 97, loss = 0.00443485\n",
      "Iteration 98, loss = 0.00437493\n",
      "Iteration 99, loss = 0.00428639\n",
      "Iteration 100, loss = 0.00419393\n",
      "Iteration 101, loss = 0.00421524\n",
      "Iteration 102, loss = 0.00416503\n",
      "Iteration 103, loss = 0.00411305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.95068684\n",
      "Iteration 2, loss = 0.28595594\n",
      "Iteration 3, loss = 0.19342765\n",
      "Iteration 4, loss = 0.15472971\n",
      "Iteration 5, loss = 0.12268467\n",
      "Iteration 6, loss = 0.10303118\n",
      "Iteration 7, loss = 0.08734713\n",
      "Iteration 8, loss = 0.07521633\n",
      "Iteration 9, loss = 0.06615454\n",
      "Iteration 10, loss = 0.05976028\n",
      "Iteration 11, loss = 0.05215787\n",
      "Iteration 12, loss = 0.04777438\n",
      "Iteration 13, loss = 0.04561970\n",
      "Iteration 14, loss = 0.04055627\n",
      "Iteration 15, loss = 0.03706443\n",
      "Iteration 16, loss = 0.03325465\n",
      "Iteration 17, loss = 0.03294425\n",
      "Iteration 18, loss = 0.03031334\n",
      "Iteration 19, loss = 0.02760724\n",
      "Iteration 20, loss = 0.02631027\n",
      "Iteration 21, loss = 0.02498167\n",
      "Iteration 22, loss = 0.02372608\n",
      "Iteration 23, loss = 0.02253378\n",
      "Iteration 24, loss = 0.02077840\n",
      "Iteration 25, loss = 0.01997315\n",
      "Iteration 26, loss = 0.01983127\n",
      "Iteration 27, loss = 0.01846922\n",
      "Iteration 28, loss = 0.01778356\n",
      "Iteration 29, loss = 0.01743313\n",
      "Iteration 30, loss = 0.01667214\n",
      "Iteration 31, loss = 0.01545938\n",
      "Iteration 32, loss = 0.01503068\n",
      "Iteration 33, loss = 0.01463789\n",
      "Iteration 34, loss = 0.01387902\n",
      "Iteration 35, loss = 0.01436342\n",
      "Iteration 36, loss = 0.01346659\n",
      "Iteration 37, loss = 0.01288319\n",
      "Iteration 38, loss = 0.01239268\n",
      "Iteration 39, loss = 0.01206870\n",
      "Iteration 40, loss = 0.01181290\n",
      "Iteration 41, loss = 0.01147060\n",
      "Iteration 42, loss = 0.01126181\n",
      "Iteration 43, loss = 0.01115861\n",
      "Iteration 44, loss = 0.01108544\n",
      "Iteration 45, loss = 0.01080437\n",
      "Iteration 46, loss = 0.01033977\n",
      "Iteration 47, loss = 0.00980613\n",
      "Iteration 48, loss = 0.00969063\n",
      "Iteration 49, loss = 0.00946604\n",
      "Iteration 50, loss = 0.00951977\n",
      "Iteration 51, loss = 0.00900549\n",
      "Iteration 52, loss = 0.00889121\n",
      "Iteration 53, loss = 0.00877397\n",
      "Iteration 54, loss = 0.00856784\n",
      "Iteration 55, loss = 0.00819432\n",
      "Iteration 56, loss = 0.00814656\n",
      "Iteration 57, loss = 0.00806329\n",
      "Iteration 58, loss = 0.00786377\n",
      "Iteration 59, loss = 0.00771337\n",
      "Iteration 60, loss = 0.00757258\n",
      "Iteration 61, loss = 0.00747316\n",
      "Iteration 62, loss = 0.00729051\n",
      "Iteration 63, loss = 0.00713828\n",
      "Iteration 64, loss = 0.00707234\n",
      "Iteration 65, loss = 0.00695693\n",
      "Iteration 66, loss = 0.00701001\n",
      "Iteration 67, loss = 0.00683952\n",
      "Iteration 68, loss = 0.00664604\n",
      "Iteration 69, loss = 0.00653412\n",
      "Iteration 70, loss = 0.00645260\n",
      "Iteration 71, loss = 0.00624230\n",
      "Iteration 72, loss = 0.00631457\n",
      "Iteration 73, loss = 0.00607893\n",
      "Iteration 74, loss = 0.00604434\n",
      "Iteration 75, loss = 0.00598671\n",
      "Iteration 76, loss = 0.00586807\n",
      "Iteration 77, loss = 0.00575686\n",
      "Iteration 78, loss = 0.00576748\n",
      "Iteration 79, loss = 0.00571436\n",
      "Iteration 80, loss = 0.00564371\n",
      "Iteration 81, loss = 0.00549941\n",
      "Iteration 82, loss = 0.00542474\n",
      "Iteration 83, loss = 0.00529565\n",
      "Iteration 84, loss = 0.00536832\n",
      "Iteration 85, loss = 0.00518241\n",
      "Iteration 86, loss = 0.00511944\n",
      "Iteration 87, loss = 0.00511122\n",
      "Iteration 88, loss = 0.00494565\n",
      "Iteration 89, loss = 0.00500311\n",
      "Iteration 90, loss = 0.00485250\n",
      "Iteration 91, loss = 0.00480938\n",
      "Iteration 92, loss = 0.00484067\n",
      "Iteration 93, loss = 0.00476081\n",
      "Iteration 94, loss = 0.00467967\n",
      "Iteration 95, loss = 0.00463911\n",
      "Iteration 96, loss = 0.00454591\n",
      "Iteration 97, loss = 0.00451711\n",
      "Iteration 98, loss = 0.00446054\n",
      "Iteration 99, loss = 0.00439203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.95769028\n",
      "Iteration 2, loss = 0.29226934\n",
      "Iteration 3, loss = 0.17616692\n",
      "Iteration 4, loss = 0.12845512\n",
      "Iteration 5, loss = 0.10348441\n",
      "Iteration 6, loss = 0.09192502\n",
      "Iteration 7, loss = 0.07447326\n",
      "Iteration 8, loss = 0.06650664\n",
      "Iteration 9, loss = 0.06314268\n",
      "Iteration 10, loss = 0.05512023\n",
      "Iteration 11, loss = 0.04974140\n",
      "Iteration 12, loss = 0.04708596\n",
      "Iteration 13, loss = 0.04164048\n",
      "Iteration 14, loss = 0.03952439\n",
      "Iteration 15, loss = 0.03663826\n",
      "Iteration 16, loss = 0.03440257\n",
      "Iteration 17, loss = 0.03274022\n",
      "Iteration 18, loss = 0.02965746\n",
      "Iteration 19, loss = 0.02978136\n",
      "Iteration 20, loss = 0.02702158\n",
      "Iteration 21, loss = 0.02481883\n",
      "Iteration 22, loss = 0.02390503\n",
      "Iteration 23, loss = 0.02379649\n",
      "Iteration 24, loss = 0.02310615\n",
      "Iteration 25, loss = 0.02082169\n",
      "Iteration 26, loss = 0.02041638\n",
      "Iteration 27, loss = 0.01987861\n",
      "Iteration 28, loss = 0.01878778\n",
      "Iteration 29, loss = 0.01784294\n",
      "Iteration 30, loss = 0.01714122\n",
      "Iteration 31, loss = 0.01672939\n",
      "Iteration 32, loss = 0.01637685\n",
      "Iteration 33, loss = 0.01561091\n",
      "Iteration 34, loss = 0.01482287\n",
      "Iteration 35, loss = 0.01508582\n",
      "Iteration 36, loss = 0.01429452\n",
      "Iteration 37, loss = 0.01323734\n",
      "Iteration 38, loss = 0.01373623\n",
      "Iteration 39, loss = 0.01341704\n",
      "Iteration 40, loss = 0.01257177\n",
      "Iteration 41, loss = 0.01237080\n",
      "Iteration 42, loss = 0.01212344\n",
      "Iteration 43, loss = 0.01173052\n",
      "Iteration 44, loss = 0.01149631\n",
      "Iteration 45, loss = 0.01097104\n",
      "Iteration 46, loss = 0.01083421\n",
      "Iteration 47, loss = 0.01054824\n",
      "Iteration 48, loss = 0.01011137\n",
      "Iteration 49, loss = 0.00990963\n",
      "Iteration 50, loss = 0.00980591\n",
      "Iteration 51, loss = 0.00948357\n",
      "Iteration 52, loss = 0.00947151\n",
      "Iteration 53, loss = 0.00920660\n",
      "Iteration 54, loss = 0.00884292\n",
      "Iteration 55, loss = 0.00881714\n",
      "Iteration 56, loss = 0.00889733\n",
      "Iteration 57, loss = 0.00840751\n",
      "Iteration 58, loss = 0.00835530\n",
      "Iteration 59, loss = 0.00821306\n",
      "Iteration 60, loss = 0.00797288\n",
      "Iteration 61, loss = 0.00791977\n",
      "Iteration 62, loss = 0.00785059\n",
      "Iteration 63, loss = 0.00762140\n",
      "Iteration 64, loss = 0.00750269\n",
      "Iteration 65, loss = 0.00730189\n",
      "Iteration 66, loss = 0.00732661\n",
      "Iteration 67, loss = 0.00717733\n",
      "Iteration 68, loss = 0.00707326\n",
      "Iteration 69, loss = 0.00688012\n",
      "Iteration 70, loss = 0.00688670\n",
      "Iteration 71, loss = 0.00679155\n",
      "Iteration 72, loss = 0.00673919\n",
      "Iteration 73, loss = 0.00660101\n",
      "Iteration 74, loss = 0.00661195\n",
      "Iteration 75, loss = 0.00629117\n",
      "Iteration 76, loss = 0.00625427\n",
      "Iteration 77, loss = 0.00620867\n",
      "Iteration 78, loss = 0.00613197\n",
      "Iteration 79, loss = 0.00598120\n",
      "Iteration 80, loss = 0.00583333\n",
      "Iteration 81, loss = 0.00587729\n",
      "Iteration 82, loss = 0.00577443\n",
      "Iteration 83, loss = 0.00564359\n",
      "Iteration 84, loss = 0.00567580\n",
      "Iteration 85, loss = 0.00544515\n",
      "Iteration 86, loss = 0.00551387\n",
      "Iteration 87, loss = 0.00533963\n",
      "Iteration 88, loss = 0.00529436\n",
      "Iteration 89, loss = 0.00537568\n",
      "Iteration 90, loss = 0.00533347\n",
      "Iteration 91, loss = 0.00516989\n",
      "Iteration 92, loss = 0.00509286\n",
      "Iteration 93, loss = 0.00514781\n",
      "Iteration 94, loss = 0.00501917\n",
      "Iteration 95, loss = 0.00492946\n",
      "Iteration 96, loss = 0.00484763\n",
      "Iteration 97, loss = 0.00479049\n",
      "Iteration 98, loss = 0.00480551\n",
      "Iteration 99, loss = 0.00467981\n",
      "Iteration 100, loss = 0.00462842\n",
      "Iteration 101, loss = 0.00460398\n",
      "Iteration 102, loss = 0.00458679\n",
      "Iteration 103, loss = 0.00446623\n",
      "Iteration 104, loss = 0.00446006\n",
      "Iteration 105, loss = 0.00437300\n",
      "Iteration 106, loss = 0.00435883\n",
      "Iteration 107, loss = 0.00432329\n",
      "Iteration 108, loss = 0.00424924\n",
      "Iteration 109, loss = 0.00422196\n",
      "Iteration 110, loss = 0.00419513\n",
      "Iteration 111, loss = 0.00417752\n",
      "Iteration 112, loss = 0.00409701\n",
      "Iteration 113, loss = 0.00415532\n",
      "Iteration 114, loss = 0.00409392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18379039\n",
      "Iteration 2, loss = 0.22768637\n",
      "Iteration 3, loss = 0.14549759\n",
      "Iteration 4, loss = 0.11147454\n",
      "Iteration 5, loss = 0.08721943\n",
      "Iteration 6, loss = 0.07259184\n",
      "Iteration 7, loss = 0.06446575\n",
      "Iteration 8, loss = 0.05664075\n",
      "Iteration 9, loss = 0.05052201\n",
      "Iteration 10, loss = 0.04365747\n",
      "Iteration 11, loss = 0.04048831\n",
      "Iteration 12, loss = 0.03592684\n",
      "Iteration 13, loss = 0.03503588\n",
      "Iteration 14, loss = 0.03108423\n",
      "Iteration 15, loss = 0.02946002\n",
      "Iteration 16, loss = 0.02760972\n",
      "Iteration 17, loss = 0.02588888\n",
      "Iteration 18, loss = 0.02532948\n",
      "Iteration 19, loss = 0.02294589\n",
      "Iteration 20, loss = 0.02270778\n",
      "Iteration 21, loss = 0.02136125\n",
      "Iteration 22, loss = 0.01994205\n",
      "Iteration 23, loss = 0.01856649\n",
      "Iteration 24, loss = 0.01811631\n",
      "Iteration 25, loss = 0.01694399\n",
      "Iteration 26, loss = 0.01642552\n",
      "Iteration 27, loss = 0.01608015\n",
      "Iteration 28, loss = 0.01493897\n",
      "Iteration 29, loss = 0.01436281\n",
      "Iteration 30, loss = 0.01347450\n",
      "Iteration 31, loss = 0.01342662\n",
      "Iteration 32, loss = 0.01274085\n",
      "Iteration 33, loss = 0.01244180\n",
      "Iteration 34, loss = 0.01173466\n",
      "Iteration 35, loss = 0.01186839\n",
      "Iteration 36, loss = 0.01123268\n",
      "Iteration 37, loss = 0.01106902\n",
      "Iteration 38, loss = 0.01077496\n",
      "Iteration 39, loss = 0.01064416\n",
      "Iteration 40, loss = 0.01003375\n",
      "Iteration 41, loss = 0.00972610\n",
      "Iteration 42, loss = 0.00983289\n",
      "Iteration 43, loss = 0.00919812\n",
      "Iteration 44, loss = 0.00936288\n",
      "Iteration 45, loss = 0.00888961\n",
      "Iteration 46, loss = 0.00884054\n",
      "Iteration 47, loss = 0.00840608\n",
      "Iteration 48, loss = 0.00832446\n",
      "Iteration 49, loss = 0.00809173\n",
      "Iteration 50, loss = 0.00800414\n",
      "Iteration 51, loss = 0.00789811\n",
      "Iteration 52, loss = 0.00753894\n",
      "Iteration 53, loss = 0.00745538\n",
      "Iteration 54, loss = 0.00722726\n",
      "Iteration 55, loss = 0.00717769\n",
      "Iteration 56, loss = 0.00701819\n",
      "Iteration 57, loss = 0.00692589\n",
      "Iteration 58, loss = 0.00670168\n",
      "Iteration 59, loss = 0.00673407\n",
      "Iteration 60, loss = 0.00645860\n",
      "Iteration 61, loss = 0.00633405\n",
      "Iteration 62, loss = 0.00621016\n",
      "Iteration 63, loss = 0.00616345\n",
      "Iteration 64, loss = 0.00605199\n",
      "Iteration 65, loss = 0.00590221\n",
      "Iteration 66, loss = 0.00583219\n",
      "Iteration 67, loss = 0.00586792\n",
      "Iteration 68, loss = 0.00572620\n",
      "Iteration 69, loss = 0.00556839\n",
      "Iteration 70, loss = 0.00550516\n",
      "Iteration 71, loss = 0.00540874\n",
      "Iteration 72, loss = 0.00538196\n",
      "Iteration 73, loss = 0.00523858\n",
      "Iteration 74, loss = 0.00515805\n",
      "Iteration 75, loss = 0.00511358\n",
      "Iteration 76, loss = 0.00506471\n",
      "Iteration 77, loss = 0.00493992\n",
      "Iteration 78, loss = 0.00485112\n",
      "Iteration 79, loss = 0.00479261\n",
      "Iteration 80, loss = 0.00483614\n",
      "Iteration 81, loss = 0.00470863\n",
      "Iteration 82, loss = 0.00475010\n",
      "Iteration 83, loss = 0.00465235\n",
      "Iteration 84, loss = 0.00451333\n",
      "Iteration 85, loss = 0.00457123\n",
      "Iteration 86, loss = 0.00436203\n",
      "Iteration 87, loss = 0.00436729\n",
      "Iteration 88, loss = 0.00431435\n",
      "Iteration 89, loss = 0.00422530\n",
      "Iteration 90, loss = 0.00418064\n",
      "Iteration 91, loss = 0.00412596\n",
      "Iteration 92, loss = 0.00407307\n",
      "Iteration 93, loss = 0.00402460\n",
      "Iteration 94, loss = 0.00400059\n",
      "Iteration 95, loss = 0.00401949\n",
      "Iteration 96, loss = 0.00391240\n",
      "Iteration 97, loss = 0.00387956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[0.96944444 0.91388889 0.96935933 0.9637883  0.91922006]\n",
      "정확률(평균)=94.714,표춘편차=0.025\n"
     ]
    }
   ],
   "source": [
    "# 다층 perceptron을 활용하여 필기 숫자 예측\n",
    "from sklearn import datasets\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 불러오기 및 훈련 데이터, 테스트 데이터 분할\n",
    "digit = datasets.load_digits()\n",
    "x_train,x_test,y_train,y_test = train_test_split(digit.data,digit.target,train_size=0.6)\n",
    "\n",
    "# 다층 perceptron 모델 학습\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=32,max_iter=300,solver='sgd',verbose=True) # 은닉층 크기 100\n",
    "mlp.fit(x_train,y_train)\n",
    "res = mlp.predict(x_test)\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "conf = np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# accuracy 측정\n",
    "not_correct = 0\n",
    "for i in range(10):\n",
    "    not_correct+=conf[i][i]\n",
    "accuracy = not_correct/len(res)\n",
    "print(\"accuracy =\",accuracy*100)\n",
    "\n",
    "# cross_val_score을 활용한 교차 검증\n",
    "accuracies = cross_val_score(mlp,digit.data,digit.target,cv=5) # 5겹 교차 검증\n",
    "print(accuracies)\n",
    "print(\"정확률(평균)=%0.3f,표춘편차=%0.3f\"%(accuracies.mean()*100,accuracies.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP(다층 퍼셉트론)은 은닉층에서 손실을 줄여나가면서 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60516375\n",
      "Iteration 2, loss = 0.25774849\n",
      "Iteration 3, loss = 0.20522300\n",
      "Iteration 4, loss = 0.17331639\n",
      "Iteration 5, loss = 0.14820280\n",
      "Iteration 6, loss = 0.13033522\n",
      "Iteration 7, loss = 0.11627765\n",
      "Iteration 8, loss = 0.10300608\n",
      "Iteration 9, loss = 0.09320511\n",
      "Iteration 10, loss = 0.08512833\n",
      "Iteration 11, loss = 0.07754887\n",
      "Iteration 12, loss = 0.07161612\n",
      "Iteration 13, loss = 0.06665915\n",
      "Iteration 14, loss = 0.06129517\n",
      "Iteration 15, loss = 0.05612737\n",
      "Iteration 16, loss = 0.05217271\n",
      "Iteration 17, loss = 0.04818715\n",
      "Iteration 18, loss = 0.04510548\n",
      "Iteration 19, loss = 0.04202745\n",
      "Iteration 20, loss = 0.03899715\n",
      "Iteration 21, loss = 0.03607322\n",
      "Iteration 22, loss = 0.03364264\n",
      "Iteration 23, loss = 0.03122141\n",
      "Iteration 24, loss = 0.02892775\n",
      "Iteration 25, loss = 0.02717587\n",
      "Iteration 26, loss = 0.02507810\n",
      "Iteration 27, loss = 0.02364340\n",
      "Iteration 28, loss = 0.02179907\n",
      "Iteration 29, loss = 0.02097004\n",
      "Iteration 30, loss = 0.01923088\n",
      "Iteration 31, loss = 0.01829394\n",
      "Iteration 32, loss = 0.01654180\n",
      "Iteration 33, loss = 0.01519692\n",
      "Iteration 34, loss = 0.01466950\n",
      "Iteration 35, loss = 0.01307963\n",
      "Iteration 36, loss = 0.01249440\n",
      "Iteration 37, loss = 0.01176604\n",
      "Iteration 38, loss = 0.01082635\n",
      "Iteration 39, loss = 0.01002230\n",
      "Iteration 40, loss = 0.00937596\n",
      "Iteration 41, loss = 0.00855382\n",
      "Iteration 42, loss = 0.00820687\n",
      "Iteration 43, loss = 0.00775552\n",
      "Iteration 44, loss = 0.00708990\n",
      "Iteration 45, loss = 0.00680284\n",
      "Iteration 46, loss = 0.00609304\n",
      "Iteration 47, loss = 0.00578201\n",
      "Iteration 48, loss = 0.00523857\n",
      "Iteration 49, loss = 0.00496939\n",
      "Iteration 50, loss = 0.00457607\n",
      "Iteration 51, loss = 0.00432247\n",
      "Iteration 52, loss = 0.00402295\n",
      "Iteration 53, loss = 0.00384143\n",
      "Iteration 54, loss = 0.00359537\n",
      "Iteration 55, loss = 0.00330370\n",
      "Iteration 56, loss = 0.00311626\n",
      "Iteration 57, loss = 0.00294686\n",
      "Iteration 58, loss = 0.00275910\n",
      "Iteration 59, loss = 0.00258600\n",
      "Iteration 60, loss = 0.00236941\n",
      "Iteration 61, loss = 0.00227509\n",
      "Iteration 62, loss = 0.00213268\n",
      "Iteration 63, loss = 0.00205492\n",
      "Iteration 64, loss = 0.00194226\n",
      "Iteration 65, loss = 0.00187123\n",
      "Iteration 66, loss = 0.00173119\n",
      "Iteration 67, loss = 0.00160819\n",
      "Iteration 68, loss = 0.00152639\n",
      "Iteration 69, loss = 0.00140425\n",
      "Iteration 70, loss = 0.00133233\n",
      "Iteration 71, loss = 0.00129687\n",
      "Iteration 72, loss = 0.00530080\n",
      "Iteration 73, loss = 0.00437781\n",
      "Iteration 74, loss = 0.00164603\n",
      "Iteration 75, loss = 0.00118582\n",
      "Iteration 76, loss = 0.00103994\n",
      "Iteration 77, loss = 0.00095208\n",
      "Iteration 78, loss = 0.00091547\n",
      "Iteration 79, loss = 0.00088841\n",
      "Iteration 80, loss = 0.00085506\n",
      "Iteration 81, loss = 0.00081932\n",
      "Iteration 82, loss = 0.00081064\n",
      "Iteration 83, loss = 0.00077779\n",
      "Iteration 84, loss = 0.00076479\n",
      "Iteration 85, loss = 0.00072542\n",
      "Iteration 86, loss = 0.00070967\n",
      "Iteration 87, loss = 0.00069178\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[[9.680e+02 0.000e+00 7.000e+00 1.000e+00 4.000e+00 1.000e+00 4.000e+00\n",
      "  2.000e+00 2.000e+00 1.000e+00]\n",
      " [0.000e+00 1.123e+03 3.000e+00 0.000e+00 0.000e+00 0.000e+00 2.000e+00\n",
      "  6.000e+00 2.000e+00 2.000e+00]\n",
      " [1.000e+00 4.000e+00 1.004e+03 3.000e+00 2.000e+00 0.000e+00 1.000e+00\n",
      "  9.000e+00 2.000e+00 0.000e+00]\n",
      " [1.000e+00 1.000e+00 3.000e+00 9.910e+02 1.000e+00 1.300e+01 1.000e+00\n",
      "  1.000e+00 6.000e+00 4.000e+00]\n",
      " [1.000e+00 0.000e+00 2.000e+00 1.000e+00 9.590e+02 2.000e+00 4.000e+00\n",
      "  3.000e+00 5.000e+00 7.000e+00]\n",
      " [2.000e+00 1.000e+00 0.000e+00 3.000e+00 1.000e+00 8.620e+02 5.000e+00\n",
      "  0.000e+00 2.000e+00 5.000e+00]\n",
      " [3.000e+00 2.000e+00 0.000e+00 0.000e+00 3.000e+00 4.000e+00 9.390e+02\n",
      "  0.000e+00 2.000e+00 1.000e+00]\n",
      " [1.000e+00 1.000e+00 5.000e+00 3.000e+00 2.000e+00 2.000e+00 1.000e+00\n",
      "  9.990e+02 5.000e+00 5.000e+00]\n",
      " [1.000e+00 3.000e+00 7.000e+00 5.000e+00 0.000e+00 7.000e+00 1.000e+00\n",
      "  3.000e+00 9.450e+02 4.000e+00]\n",
      " [2.000e+00 0.000e+00 1.000e+00 3.000e+00 1.000e+01 1.000e+00 0.000e+00\n",
      "  5.000e+00 3.000e+00 9.800e+02]]\n",
      "accuracy = 97.7\n",
      "Iteration 1, loss = 0.62131870\n",
      "Iteration 2, loss = 0.27621537\n",
      "Iteration 3, loss = 0.22400658\n",
      "Iteration 4, loss = 0.18898509\n",
      "Iteration 5, loss = 0.16185395\n",
      "Iteration 6, loss = 0.14207808\n",
      "Iteration 7, loss = 0.12557950\n",
      "Iteration 8, loss = 0.11338051\n",
      "Iteration 9, loss = 0.10261050\n",
      "Iteration 10, loss = 0.09317289\n",
      "Iteration 11, loss = 0.08538029\n",
      "Iteration 12, loss = 0.07811864\n",
      "Iteration 13, loss = 0.07191124\n",
      "Iteration 14, loss = 0.06659273\n",
      "Iteration 15, loss = 0.06189715\n",
      "Iteration 16, loss = 0.05667137\n",
      "Iteration 17, loss = 0.05277035\n",
      "Iteration 18, loss = 0.04833396\n",
      "Iteration 19, loss = 0.04527068\n",
      "Iteration 20, loss = 0.04221329\n",
      "Iteration 21, loss = 0.03893163\n",
      "Iteration 22, loss = 0.03650178\n",
      "Iteration 23, loss = 0.03390456\n",
      "Iteration 24, loss = 0.03112369\n",
      "Iteration 25, loss = 0.02936381\n",
      "Iteration 26, loss = 0.02727207\n",
      "Iteration 27, loss = 0.02527639\n",
      "Iteration 28, loss = 0.02371365\n",
      "Iteration 29, loss = 0.02191881\n",
      "Iteration 30, loss = 0.02053118\n",
      "Iteration 31, loss = 0.01946779\n",
      "Iteration 32, loss = 0.01780086\n",
      "Iteration 33, loss = 0.01665604\n",
      "Iteration 34, loss = 0.01557293\n",
      "Iteration 35, loss = 0.01457024\n",
      "Iteration 36, loss = 0.01329944\n",
      "Iteration 37, loss = 0.01273386\n",
      "Iteration 38, loss = 0.01185510\n",
      "Iteration 39, loss = 0.01078673\n",
      "Iteration 40, loss = 0.00998562\n",
      "Iteration 41, loss = 0.00930374\n",
      "Iteration 42, loss = 0.00889695\n",
      "Iteration 43, loss = 0.00811814\n",
      "Iteration 44, loss = 0.00752016\n",
      "Iteration 45, loss = 0.00691870\n",
      "Iteration 46, loss = 0.00627169\n",
      "Iteration 47, loss = 0.00603920\n",
      "Iteration 48, loss = 0.00550183\n",
      "Iteration 49, loss = 0.00513467\n",
      "Iteration 50, loss = 0.00470738\n",
      "Iteration 51, loss = 0.00451181\n",
      "Iteration 52, loss = 0.00415104\n",
      "Iteration 53, loss = 0.00386599\n",
      "Iteration 54, loss = 0.00364457\n",
      "Iteration 55, loss = 0.00337822\n",
      "Iteration 56, loss = 0.00322665\n",
      "Iteration 57, loss = 0.00302746\n",
      "Iteration 58, loss = 0.00284824\n",
      "Iteration 59, loss = 0.00266861\n",
      "Iteration 60, loss = 0.00250108\n",
      "Iteration 61, loss = 0.00232129\n",
      "Iteration 62, loss = 0.00224554\n",
      "Iteration 63, loss = 0.00214029\n",
      "Iteration 64, loss = 0.00198633\n",
      "Iteration 65, loss = 0.00201796\n",
      "Iteration 66, loss = 0.00176311\n",
      "Iteration 67, loss = 0.00165202\n",
      "Iteration 68, loss = 0.00157310\n",
      "Iteration 69, loss = 0.00152550\n",
      "Iteration 70, loss = 0.00144393\n",
      "Iteration 71, loss = 0.00140321\n",
      "Iteration 72, loss = 0.00127446\n",
      "Iteration 73, loss = 0.00122859\n",
      "Iteration 74, loss = 0.00115921\n",
      "Iteration 75, loss = 0.00111367\n",
      "Iteration 76, loss = 0.00104443\n",
      "Iteration 77, loss = 0.00099386\n",
      "Iteration 78, loss = 0.00093676\n",
      "Iteration 79, loss = 0.00094229\n",
      "Iteration 80, loss = 0.00087593\n",
      "Iteration 81, loss = 0.00084835\n",
      "Iteration 82, loss = 0.00080127\n",
      "Iteration 83, loss = 0.00076951\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66072721\n",
      "Iteration 2, loss = 0.26490061\n",
      "Iteration 3, loss = 0.20704585\n",
      "Iteration 4, loss = 0.17332955\n",
      "Iteration 5, loss = 0.14892108\n",
      "Iteration 6, loss = 0.13067239\n",
      "Iteration 7, loss = 0.11637800\n",
      "Iteration 8, loss = 0.10444330\n",
      "Iteration 9, loss = 0.09481181\n",
      "Iteration 10, loss = 0.08681050\n",
      "Iteration 11, loss = 0.07895529\n",
      "Iteration 12, loss = 0.07328720\n",
      "Iteration 13, loss = 0.06675631\n",
      "Iteration 14, loss = 0.06179051\n",
      "Iteration 15, loss = 0.05742973\n",
      "Iteration 16, loss = 0.05360301\n",
      "Iteration 17, loss = 0.05017247\n",
      "Iteration 18, loss = 0.04602055\n",
      "Iteration 19, loss = 0.04278460\n",
      "Iteration 20, loss = 0.04050964\n",
      "Iteration 21, loss = 0.03709289\n",
      "Iteration 22, loss = 0.03481097\n",
      "Iteration 23, loss = 0.03220604\n",
      "Iteration 24, loss = 0.03037015\n",
      "Iteration 25, loss = 0.02822369\n",
      "Iteration 26, loss = 0.02627381\n",
      "Iteration 27, loss = 0.02443663\n",
      "Iteration 28, loss = 0.02329502\n",
      "Iteration 29, loss = 0.02111680\n",
      "Iteration 30, loss = 0.01977230\n",
      "Iteration 31, loss = 0.01873571\n",
      "Iteration 32, loss = 0.01740160\n",
      "Iteration 33, loss = 0.01650792\n",
      "Iteration 34, loss = 0.01513677\n",
      "Iteration 35, loss = 0.01435177\n",
      "Iteration 36, loss = 0.01341294\n",
      "Iteration 37, loss = 0.01237950\n",
      "Iteration 38, loss = 0.01156237\n",
      "Iteration 39, loss = 0.01122332\n",
      "Iteration 40, loss = 0.01019120\n",
      "Iteration 41, loss = 0.00945469\n",
      "Iteration 42, loss = 0.00897195\n",
      "Iteration 43, loss = 0.00830863\n",
      "Iteration 44, loss = 0.00775888\n",
      "Iteration 45, loss = 0.00727763\n",
      "Iteration 46, loss = 0.00686930\n",
      "Iteration 47, loss = 0.00618994\n",
      "Iteration 48, loss = 0.00587553\n",
      "Iteration 49, loss = 0.00534834\n",
      "Iteration 50, loss = 0.00487904\n",
      "Iteration 51, loss = 0.00479192\n",
      "Iteration 52, loss = 0.00434465\n",
      "Iteration 53, loss = 0.00412005\n",
      "Iteration 54, loss = 0.00394670\n",
      "Iteration 55, loss = 0.00365044\n",
      "Iteration 56, loss = 0.00351276\n",
      "Iteration 57, loss = 0.00327109\n",
      "Iteration 58, loss = 0.00313770\n",
      "Iteration 59, loss = 0.00288817\n",
      "Iteration 60, loss = 0.00275974\n",
      "Iteration 61, loss = 0.00251607\n",
      "Iteration 62, loss = 0.00237905\n",
      "Iteration 63, loss = 0.00225340\n",
      "Iteration 64, loss = 0.00208923\n",
      "Iteration 65, loss = 0.00204408\n",
      "Iteration 66, loss = 0.00192679\n",
      "Iteration 67, loss = 0.00180068\n",
      "Iteration 68, loss = 0.00167139\n",
      "Iteration 69, loss = 0.00162110\n",
      "Iteration 70, loss = 0.00149459\n",
      "Iteration 71, loss = 0.00151973\n",
      "Iteration 72, loss = 0.00143745\n",
      "Iteration 73, loss = 0.00128847\n",
      "Iteration 74, loss = 0.00122912\n",
      "Iteration 75, loss = 0.00117807\n",
      "Iteration 76, loss = 0.00111701\n",
      "Iteration 77, loss = 0.00108846\n",
      "Iteration 78, loss = 0.00103852\n",
      "Iteration 79, loss = 0.00098290\n",
      "Iteration 80, loss = 0.00093468\n",
      "Iteration 81, loss = 0.00092261\n",
      "Iteration 82, loss = 0.00087522\n",
      "Iteration 83, loss = 0.00083420\n",
      "Iteration 84, loss = 0.00078520\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63948337\n",
      "Iteration 2, loss = 0.26672517\n",
      "Iteration 3, loss = 0.21571116\n",
      "Iteration 4, loss = 0.18236761\n",
      "Iteration 5, loss = 0.15858366\n",
      "Iteration 6, loss = 0.14043633\n",
      "Iteration 7, loss = 0.12450846\n",
      "Iteration 8, loss = 0.11192895\n",
      "Iteration 9, loss = 0.10101509\n",
      "Iteration 10, loss = 0.09144986\n",
      "Iteration 11, loss = 0.08354258\n",
      "Iteration 12, loss = 0.07589626\n",
      "Iteration 13, loss = 0.06966314\n",
      "Iteration 14, loss = 0.06397130\n",
      "Iteration 15, loss = 0.05928747\n",
      "Iteration 16, loss = 0.05421000\n",
      "Iteration 17, loss = 0.05059276\n",
      "Iteration 18, loss = 0.04681929\n",
      "Iteration 19, loss = 0.04339970\n",
      "Iteration 20, loss = 0.03987652\n",
      "Iteration 21, loss = 0.03742500\n",
      "Iteration 22, loss = 0.03440295\n",
      "Iteration 23, loss = 0.03195523\n",
      "Iteration 24, loss = 0.02966092\n",
      "Iteration 25, loss = 0.02778963\n",
      "Iteration 26, loss = 0.02580282\n",
      "Iteration 27, loss = 0.02395970\n",
      "Iteration 28, loss = 0.02219626\n",
      "Iteration 29, loss = 0.02058223\n",
      "Iteration 30, loss = 0.01953678\n",
      "Iteration 31, loss = 0.01783871\n",
      "Iteration 32, loss = 0.01681299\n",
      "Iteration 33, loss = 0.01567916\n",
      "Iteration 34, loss = 0.01451910\n",
      "Iteration 35, loss = 0.01345022\n",
      "Iteration 36, loss = 0.01231838\n",
      "Iteration 37, loss = 0.01162981\n",
      "Iteration 38, loss = 0.01084147\n",
      "Iteration 39, loss = 0.01008692\n",
      "Iteration 40, loss = 0.00933862\n",
      "Iteration 41, loss = 0.00878314\n",
      "Iteration 42, loss = 0.00816943\n",
      "Iteration 43, loss = 0.00782872\n",
      "Iteration 44, loss = 0.00721900\n",
      "Iteration 45, loss = 0.00672539\n",
      "Iteration 46, loss = 0.00608048\n",
      "Iteration 47, loss = 0.00570408\n",
      "Iteration 48, loss = 0.00546267\n",
      "Iteration 49, loss = 0.00516779\n",
      "Iteration 50, loss = 0.00468993\n",
      "Iteration 51, loss = 0.00446288\n",
      "Iteration 52, loss = 0.00412316\n",
      "Iteration 53, loss = 0.00382270\n",
      "Iteration 54, loss = 0.00359285\n",
      "Iteration 55, loss = 0.00342953\n",
      "Iteration 56, loss = 0.00330303\n",
      "Iteration 57, loss = 0.00284898\n",
      "Iteration 58, loss = 0.00276308\n",
      "Iteration 59, loss = 0.00254792\n",
      "Iteration 60, loss = 0.00240672\n",
      "Iteration 61, loss = 0.00220394\n",
      "Iteration 62, loss = 0.00224003\n",
      "Iteration 63, loss = 0.00198261\n",
      "Iteration 64, loss = 0.00188733\n",
      "Iteration 65, loss = 0.00180795\n",
      "Iteration 66, loss = 0.00194984\n",
      "Iteration 67, loss = 0.00168167\n",
      "Iteration 68, loss = 0.00152645\n",
      "Iteration 69, loss = 0.00146845\n",
      "Iteration 70, loss = 0.00134255\n",
      "Iteration 71, loss = 0.00130385\n",
      "Iteration 72, loss = 0.00124635\n",
      "Iteration 73, loss = 0.00119211\n",
      "Iteration 74, loss = 0.00110881\n",
      "Iteration 75, loss = 0.00106028\n",
      "Iteration 76, loss = 0.00101798\n",
      "Iteration 77, loss = 0.00094617\n",
      "Iteration 78, loss = 0.00090945\n",
      "Iteration 79, loss = 0.00087412\n",
      "Iteration 80, loss = 0.00085405\n",
      "Iteration 81, loss = 0.00080823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63966760\n",
      "Iteration 2, loss = 0.27178892\n",
      "Iteration 3, loss = 0.21198293\n",
      "Iteration 4, loss = 0.17540168\n",
      "Iteration 5, loss = 0.14878324\n",
      "Iteration 6, loss = 0.13040059\n",
      "Iteration 7, loss = 0.11436751\n",
      "Iteration 8, loss = 0.10250025\n",
      "Iteration 9, loss = 0.09286766\n",
      "Iteration 10, loss = 0.08372218\n",
      "Iteration 11, loss = 0.07590899\n",
      "Iteration 12, loss = 0.06948238\n",
      "Iteration 13, loss = 0.06451923\n",
      "Iteration 14, loss = 0.05962331\n",
      "Iteration 15, loss = 0.05531815\n",
      "Iteration 16, loss = 0.05061251\n",
      "Iteration 17, loss = 0.04705677\n",
      "Iteration 18, loss = 0.04451344\n",
      "Iteration 19, loss = 0.04104884\n",
      "Iteration 20, loss = 0.03800458\n",
      "Iteration 21, loss = 0.03541566\n",
      "Iteration 22, loss = 0.03338031\n",
      "Iteration 23, loss = 0.03111280\n",
      "Iteration 24, loss = 0.02896812\n",
      "Iteration 25, loss = 0.02731388\n",
      "Iteration 26, loss = 0.02559401\n",
      "Iteration 27, loss = 0.02357600\n",
      "Iteration 28, loss = 0.02188450\n",
      "Iteration 29, loss = 0.02029775\n",
      "Iteration 30, loss = 0.01888553\n",
      "Iteration 31, loss = 0.01773676\n",
      "Iteration 32, loss = 0.01678085\n",
      "Iteration 33, loss = 0.01577505\n",
      "Iteration 34, loss = 0.01442646\n",
      "Iteration 35, loss = 0.01338543\n",
      "Iteration 36, loss = 0.01221443\n",
      "Iteration 37, loss = 0.01186508\n",
      "Iteration 38, loss = 0.01094460\n",
      "Iteration 39, loss = 0.01012972\n",
      "Iteration 40, loss = 0.00952733\n",
      "Iteration 41, loss = 0.00882053\n",
      "Iteration 42, loss = 0.00854272\n",
      "Iteration 43, loss = 0.00753463\n",
      "Iteration 44, loss = 0.00710890\n",
      "Iteration 45, loss = 0.00693914\n",
      "Iteration 46, loss = 0.00623486\n",
      "Iteration 47, loss = 0.00616162\n",
      "Iteration 48, loss = 0.00557115\n",
      "Iteration 49, loss = 0.00503795\n",
      "Iteration 50, loss = 0.00485924\n",
      "Iteration 51, loss = 0.00455547\n",
      "Iteration 52, loss = 0.00404752\n",
      "Iteration 53, loss = 0.00391569\n",
      "Iteration 54, loss = 0.00365876\n",
      "Iteration 55, loss = 0.00344970\n",
      "Iteration 56, loss = 0.00363424\n",
      "Iteration 57, loss = 0.00302560\n",
      "Iteration 58, loss = 0.00286073\n",
      "Iteration 59, loss = 0.00272391\n",
      "Iteration 60, loss = 0.00281277\n",
      "Iteration 61, loss = 0.00256496\n",
      "Iteration 62, loss = 0.00225793\n",
      "Iteration 63, loss = 0.00209619\n",
      "Iteration 64, loss = 0.00207092\n",
      "Iteration 65, loss = 0.00194995\n",
      "Iteration 66, loss = 0.00189246\n",
      "Iteration 67, loss = 0.00169392\n",
      "Iteration 68, loss = 0.00156792\n",
      "Iteration 69, loss = 0.00152468\n",
      "Iteration 70, loss = 0.00141829\n",
      "Iteration 71, loss = 0.00137496\n",
      "Iteration 72, loss = 0.00132620\n",
      "Iteration 73, loss = 0.00127087\n",
      "Iteration 74, loss = 0.00121347\n",
      "Iteration 75, loss = 0.00113121\n",
      "Iteration 76, loss = 0.00106500\n",
      "Iteration 77, loss = 0.00104103\n",
      "Iteration 78, loss = 0.00102250\n",
      "Iteration 79, loss = 0.00096765\n",
      "Iteration 80, loss = 0.00090784\n",
      "Iteration 81, loss = 0.00084983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67119507\n",
      "Iteration 2, loss = 0.28012715\n",
      "Iteration 3, loss = 0.22180095\n",
      "Iteration 4, loss = 0.18539001\n",
      "Iteration 5, loss = 0.15905941\n",
      "Iteration 6, loss = 0.13991952\n",
      "Iteration 7, loss = 0.12462946\n",
      "Iteration 8, loss = 0.11164804\n",
      "Iteration 9, loss = 0.10144364\n",
      "Iteration 10, loss = 0.09203004\n",
      "Iteration 11, loss = 0.08420260\n",
      "Iteration 12, loss = 0.07752515\n",
      "Iteration 13, loss = 0.07185920\n",
      "Iteration 14, loss = 0.06612807\n",
      "Iteration 15, loss = 0.06149962\n",
      "Iteration 16, loss = 0.05638773\n",
      "Iteration 17, loss = 0.05266303\n",
      "Iteration 18, loss = 0.04959569\n",
      "Iteration 19, loss = 0.04595874\n",
      "Iteration 20, loss = 0.04300357\n",
      "Iteration 21, loss = 0.03997644\n",
      "Iteration 22, loss = 0.03696667\n",
      "Iteration 23, loss = 0.03491779\n",
      "Iteration 24, loss = 0.03209425\n",
      "Iteration 25, loss = 0.03027613\n",
      "Iteration 26, loss = 0.02813367\n",
      "Iteration 27, loss = 0.02648504\n",
      "Iteration 28, loss = 0.02445422\n",
      "Iteration 29, loss = 0.02289537\n",
      "Iteration 30, loss = 0.02136725\n",
      "Iteration 31, loss = 0.02031038\n",
      "Iteration 32, loss = 0.01910845\n",
      "Iteration 33, loss = 0.01733042\n",
      "Iteration 34, loss = 0.01633136\n",
      "Iteration 35, loss = 0.01505530\n",
      "Iteration 36, loss = 0.01409653\n",
      "Iteration 37, loss = 0.01332934\n",
      "Iteration 38, loss = 0.01248734\n",
      "Iteration 39, loss = 0.01169557\n",
      "Iteration 40, loss = 0.01082186\n",
      "Iteration 41, loss = 0.01043036\n",
      "Iteration 42, loss = 0.00949861\n",
      "Iteration 43, loss = 0.00866351\n",
      "Iteration 44, loss = 0.00824377\n",
      "Iteration 45, loss = 0.00803962\n",
      "Iteration 46, loss = 0.00725178\n",
      "Iteration 47, loss = 0.00702262\n",
      "Iteration 48, loss = 0.00633875\n",
      "Iteration 49, loss = 0.00595378\n",
      "Iteration 50, loss = 0.00550072\n",
      "Iteration 51, loss = 0.00519821\n",
      "Iteration 52, loss = 0.00478693\n",
      "Iteration 53, loss = 0.00448043\n",
      "Iteration 54, loss = 0.00424481\n",
      "Iteration 55, loss = 0.00389768\n",
      "Iteration 56, loss = 0.00361825\n",
      "Iteration 57, loss = 0.00337163\n",
      "Iteration 58, loss = 0.00325194\n",
      "Iteration 59, loss = 0.00304597\n",
      "Iteration 60, loss = 0.00288327\n",
      "Iteration 61, loss = 0.00272435\n",
      "Iteration 62, loss = 0.00261261\n",
      "Iteration 63, loss = 0.00244518\n",
      "Iteration 64, loss = 0.00224746\n",
      "Iteration 65, loss = 0.00226339\n",
      "Iteration 66, loss = 0.00209947\n",
      "Iteration 67, loss = 0.00188906\n",
      "Iteration 68, loss = 0.00180720\n",
      "Iteration 69, loss = 0.00168893\n",
      "Iteration 70, loss = 0.00159461\n",
      "Iteration 71, loss = 0.00160367\n",
      "Iteration 72, loss = 0.00148823\n",
      "Iteration 73, loss = 0.00137070\n",
      "Iteration 74, loss = 0.00132280\n",
      "Iteration 75, loss = 0.00126194\n",
      "Iteration 76, loss = 0.00121804\n",
      "Iteration 77, loss = 0.00119931\n",
      "Iteration 78, loss = 0.00110453\n",
      "Iteration 79, loss = 0.00106497\n",
      "Iteration 80, loss = 0.00099925\n",
      "Iteration 81, loss = 0.00095616\n",
      "Iteration 82, loss = 0.00090412\n",
      "Iteration 83, loss = 0.00086000\n",
      "Iteration 84, loss = 0.00083330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[0.97757143 0.97478571 0.9735     0.97442857 0.97878571]\n",
      "정확률(평균)=97.581,표춘편차=0.002\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터셋을 활용하여 다층 퍼셉트론으로 필기체 예측\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# MNIST 데이터셋 불러오기 및 훈련 집합과 테스트 집합 분할\n",
    "mnist = fetch_openml('mnist_784')\n",
    "mnist.data = mnist.data/255.0 # MNIST의 데이터셋은 0~255의 범위값을 가지므로 255로 나눠서 데이터 정규화\n",
    "x_train = mnist.data[:60000]\n",
    "y_train = np.int16(mnist.target[:60000])\n",
    "x_test = mnist.data[60000:]\n",
    "y_test = np.int16(mnist.target[60000:])\n",
    "\n",
    "# MLP를 통해 모델 학습\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100),learning_rate_init=0.001,batch_size=512,max_iter=300,solver='adam',verbose=True)\n",
    "# 은닉층 노드 100개, 학습률 0.001 verbose=True -> 학습 과정 출력\n",
    "mlp.fit(x_train,y_train)\n",
    "res = mlp.predict(x_test)\n",
    "\n",
    "# 혼동 행렬 생성\n",
    "conf = np.zeros((10,10))\n",
    "for i in range(len(res)):\n",
    "    conf[res[i]][y_test[i]]+=1\n",
    "print(conf)\n",
    "\n",
    "# accuracy 측정\n",
    "not_correct = 0\n",
    "for i in range(10):\n",
    "    not_correct+=conf[i][i]\n",
    "accuracy = not_correct/len(res)\n",
    "print(\"accuracy =\",accuracy*100)\n",
    "\n",
    "# cross_val_score을 활용한 교차 검증\n",
    "accuracies = cross_val_score(mlp,mnist.data,mnist.target,cv=5) # 5겹 교차 검증\n",
    "print(accuracies)\n",
    "print(\"정확률(평균)=%0.3f,표춘편차=%0.3f\"%(accuracies.mean()*100,accuracies.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
